{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec3f5dd3",
   "metadata": {},
   "source": [
    "# Notebook 01: Data Collection from ArXiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c5dd45",
   "metadata": {},
   "source": [
    "###   Scrape research papers and create a dataset for RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34cedfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287c487b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data collection at 2025-09-29 14:46:42.030279\n",
      "Target: 5000 papers\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 1. CONFIGURATION\n",
    "# ================================\n",
    "\n",
    "\n",
    "SEARCH_QUERIES = [\n",
    "    \"machine learning\",\n",
    "    \"deep learning\",\n",
    "    \"natural language processing\",\n",
    "    \"computer vision\",\n",
    "    \"reinforcement learning\"]\n",
    "\n",
    "MAX_RESULTS_PER_QUERY = 1000  \n",
    "CATEGORIES = ['cs.LG', 'cs.AI', 'cs.CL', 'cs.CV']  # Relevant CS categories\n",
    "OUTPUT_PATH = '../data/arxiv_papers.csv'\n",
    "\n",
    "print(f\"Starting data collection at {datetime.now()}\")\n",
    "print(f\"Target: {MAX_RESULTS_PER_QUERY * len(SEARCH_QUERIES)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 2. DATA COLLECTION FUNCTION - CORRECTED\n",
    "# ================================\n",
    "\n",
    "def fetch_arxiv_papers(query, max_results=1000, max_retries=3):\n",
    "    \"\"\"\n",
    "    Fetch papers from ArXiv API with retry logic.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        max_results: Maximum number of papers to fetch\n",
    "        max_retries: Number of times to retry on a temporary failure\n",
    "    \n",
    "    Returns:\n",
    "        List of paper dictionaries\n",
    "    \"\"\"\n",
    "    \n",
    "    papers = []\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Recreate the Search object for each attempt in case of connection issues\n",
    "            search = arxiv.Search(\n",
    "                query=query,\n",
    "                max_results=max_results,\n",
    "                sort_by=arxiv.SortCriterion.Relevance,\n",
    "                sort_order=arxiv.SortOrder.Descending\n",
    "            )\n",
    "            \n",
    "            # The search.results() generator will raise the error when it runs out of pages.\n",
    "            for result in search.results():\n",
    "                paper = {\n",
    "                    'arxiv_id': result.entry_id.split('/')[-1],\n",
    "                    'title': result.title,\n",
    "                    'abstract': result.summary.replace('\\n', ' '),\n",
    "                    'authors': ', '.join([author.name for author in result.authors]),\n",
    "                    'published_date': result.published.strftime('%Y-%m-%d'),\n",
    "                    'categories': ', '.join(result.categories),\n",
    "                    'pdf_url': result.pdf_url,\n",
    "                    'query': query\n",
    "                }\n",
    "                papers.append(paper)\n",
    "            \n",
    "            # If the loop finishes without an error, we successfully fetched all max_results papers.\n",
    "            return papers\n",
    "\n",
    "      \n",
    "        except arxiv.UnexpectedEmptyPageError as e:\n",
    "            # Handle the specific case where max_results was requested,but the API returned fewer papers (ran out of results).\n",
    "            print(f\"   ‚ÑπÔ∏è Query '{query}' ran out of results. Fetched {len(papers)} papers.\")\n",
    "            return papers # Return the papers collected so far.\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Handle genuine API/network/parsing errors \n",
    "            print(f\"   ‚ö†Ô∏è Attempt {attempt + 1} failed for query '{query}': {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                # Wait longer between retries for a genuine error\n",
    "                sleep_time = 5 * (attempt + 1)\n",
    "                print(f\"   ‚è≥ Retrying in {sleep_time} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                print(f\"   ‚ùå Max retries reached for query '{query}'. Skipping.\")\n",
    "                return [] # Return empty list on final failure\n",
    "    \n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d520a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching papers:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for: 'machine learning'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_1216\\1269289785.py:34: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ÑπÔ∏è Query 'machine learning' ran out of results. Fetched 500 papers.\n",
      "   ‚úì Found 500 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching papers:  20%|‚ñà‚ñà        | 1/5 [00:26<01:46, 26.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for: 'deep learning'\n",
      "   ‚ÑπÔ∏è Query 'deep learning' ran out of results. Fetched 500 papers.\n",
      "   ‚úì Found 500 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching papers:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:53<01:19, 26.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for: 'natural language processing'\n",
      "   ‚ÑπÔ∏è Query 'natural language processing' ran out of results. Fetched 100 papers.\n",
      "   ‚úì Found 100 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching papers:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [01:06<00:41, 20.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for: 'computer vision'\n",
      "   ‚ÑπÔ∏è Query 'computer vision' ran out of results. Fetched 200 papers.\n",
      "   ‚úì Found 200 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching papers:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [01:23<00:19, 19.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for: 'reinforcement learning'\n",
      "   ‚úì Found 1000 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching papers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:52<00:00, 22.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Total papers collected: 2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 3. COLLECT DATA\n",
    "# ================================\n",
    "\n",
    "all_papers = []\n",
    "\n",
    "for query in tqdm(SEARCH_QUERIES, desc=\"Fetching papers\"):\n",
    "    print(f\"\\nüîç Searching for: '{query}'\")\n",
    "    papers = fetch_arxiv_papers(query, MAX_RESULTS_PER_QUERY)\n",
    "    all_papers.extend(papers)\n",
    "    print(f\"   ‚úì Found {len(papers)} papers\")\n",
    "    time.sleep(1)  # Be nice to the API\n",
    "\n",
    "print(f\"\\nüìä Total papers collected: {len(all_papers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0881403c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before deduplication: 2300 papers\n",
      "After deduplication: 2228 papers\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 4. CREATE DATAFRAME\n",
    "# ================================\n",
    "\n",
    "df = pd.DataFrame(all_papers)\n",
    "\n",
    "# Remove duplicates (same paper might appear in multiple queries)\n",
    "print(f\"\\nBefore deduplication: {len(df)} papers\")\n",
    "df = df.drop_duplicates(subset=['arxiv_id'])\n",
    "print(f\"After deduplication: {len(df)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2ff232f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA QUALITY ASSESSMENT\n",
      "============================================================\n",
      "\n",
      "Missing values:\n",
      "arxiv_id          0\n",
      "title             0\n",
      "abstract          0\n",
      "authors           0\n",
      "published_date    0\n",
      "categories        0\n",
      "pdf_url           0\n",
      "query             0\n",
      "dtype: int64\n",
      "\n",
      "Abstract length statistics:\n",
      "count    2228.000000\n",
      "mean      981.642729\n",
      "std       390.430930\n",
      "min         3.000000\n",
      "25%       680.000000\n",
      "50%      1000.500000\n",
      "75%      1252.250000\n",
      "max      1919.000000\n",
      "Name: abstract_length, dtype: float64\n",
      "\n",
      "Top 10 categories:\n",
      "categories\n",
      "cs.LG      1614\n",
      "cs.AI       765\n",
      "stat.ML     660\n",
      "cs.CV       319\n",
      "cs.RO       172\n",
      "cs.CL       154\n",
      "cs.SY        89\n",
      "cs.NE        84\n",
      "eess.SY      82\n",
      "cs.CR        59\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Papers by year:\n",
      "year\n",
      "2025    146\n",
      "2024    231\n",
      "2023    243\n",
      "2022    224\n",
      "2021    228\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 5. BASIC DATA QUALITY CHECKS\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Abstract length statistics\n",
    "df['abstract_length'] = df['abstract'].str.len()\n",
    "print(\"\\nAbstract length statistics:\")\n",
    "print(df['abstract_length'].describe())\n",
    "\n",
    "# Papers per category\n",
    "print(\"\\nTop 10 categories:\")\n",
    "all_categories = df['categories'].str.split(', ').explode()\n",
    "print(all_categories.value_counts().head(10))\n",
    "\n",
    "# Papers over time\n",
    "df['year'] = pd.to_datetime(df['published_date']).dt.year\n",
    "print(\"\\nPapers by year:\")\n",
    "print(df['year'].value_counts().sort_index(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43f771fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Data saved to ../data/arxiv_papers.csv\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 6. SAVE DATA\n",
    "# ================================\n",
    "\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"\\n‚úÖ Data saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36d2e32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAMPLE PAPERS\n",
      "============================================================\n",
      "\n",
      "üìÑ Paper 1:\n",
      "   Title: Lecture Notes: Optimization for Machine Learning...\n",
      "   Authors: Elad Hazan...\n",
      "   Categories: cs.LG, stat.ML\n",
      "   Abstract: Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well a...\n",
      "\n",
      "üìÑ Paper 2:\n",
      "   Title: An Optimal Control View of Adversarial Machine Learning...\n",
      "   Authors: Xiaojin Zhu...\n",
      "   Categories: cs.LG, stat.ML\n",
      "   Abstract: I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actio...\n",
      "\n",
      "üìÑ Paper 3:\n",
      "   Title: Minimax deviation strategies for machine learning and recognition with short lea...\n",
      "   Authors: Michail Schlesinger, Evgeniy Vodolazskiy...\n",
      "   Categories: cs.LG\n",
      "   Abstract: The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are...\n",
      "\n",
      "============================================================\n",
      "DATA COLLECTION COMPLETE!\n",
      "Dataset shape: (2228, 10)\n",
      "Ready for EDA in notebook 02\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================\n",
    "# 7. SAMPLE PREVIEW\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE PAPERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for idx, row in df.head(3).iterrows():\n",
    "    print(f\"\\nüìÑ Paper {idx + 1}:\")\n",
    "    print(f\"   Title: {row['title'][:80]}...\")\n",
    "    print(f\"   Authors: {row['authors'][:60]}...\")\n",
    "    print(f\"   Categories: {row['categories']}\")\n",
    "    print(f\"   Abstract: {row['abstract'][:150]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA COLLECTION COMPLETE!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Ready for EDA in notebook 02\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1160cf08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_en",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
