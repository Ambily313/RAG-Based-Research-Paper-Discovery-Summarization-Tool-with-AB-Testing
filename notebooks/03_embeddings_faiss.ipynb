{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f837729a",
   "metadata": {},
   "source": [
    "# Notebook 03: Embedding Generation & FAISS Index Creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5edf8e",
   "metadata": {},
   "source": [
    "### Convert text to vectors and build semantic search infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f99ed81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding\\Repo\\RAG-Based-Research-Paper-Discovery-Summarization-Tool-with-AB-Testing\\rag_en\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b65ddccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Loading cleaned dataset...\n",
      "✓ Loaded 2130 papers\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 1. LOAD CLEANED DATA\n",
    "# ================================\n",
    "\n",
    "print(\"\\n📂 Loading cleaned dataset...\")\n",
    "df = pd.read_csv('../data/arxiv_papers_clean.csv')\n",
    "print(f\"✓ Loaded {len(df)} papers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "889d8c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading embedding model...\n",
      "Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "  - Dimension: 384\n",
      "  - Fast and efficient for semantic search\n",
      "  - Good balance of quality and speed\n",
      "✓ Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 2. MODEL SELECTION & LOADING\n",
    "# ================================\n",
    "\n",
    "print(\"\\n Loading embedding model...\")\n",
    "print(\"Model: sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"  - Dimension: 384\")\n",
    "print(\"  - Fast and efficient for semantic search\")\n",
    "print(\"  - Good balance of quality and speed\")\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"✓ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "541524be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATING EMBEDDINGS\n",
      "============================================================\n",
      "\n",
      "Generating embeddings for 2130 abstracts...\n",
      "This may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 67/67 [00:56<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Embedding generation complete!\n",
      "  Time taken: 56.48 seconds\n",
      "  Average time per paper: 0.027 seconds\n",
      "  Embedding shape: (2130, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 3. EMBEDDING GENERATION\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING EMBEDDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Decision: Embed abstracts (not titles) since they contain more semantic information\n",
    "texts_to_embed = df['abstract_clean'].tolist()\n",
    "\n",
    "print(f\"\\nGenerating embeddings for {len(texts_to_embed)} abstracts...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate embeddings in batches for efficiency\n",
    "batch_size = 32\n",
    "embeddings = model.encode(\n",
    "    texts_to_embed,\n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Embedding generation complete!\")\n",
    "print(f\"  Time taken: {elapsed_time:.2f} seconds\")\n",
    "print(f\"  Average time per paper: {elapsed_time/len(texts_to_embed):.3f} seconds\")\n",
    "print(f\"  Embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfceb00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EMBEDDING VALIDATION\n",
      "============================================================\n",
      "\n",
      "Embedding statistics:\n",
      "  Mean: 0.0010\n",
      "  Std: 0.0510\n",
      "  Min: -0.2439\n",
      "  Max: 0.2378\n",
      "\n",
      "Data quality checks:\n",
      "  NaN values: 0\n",
      "  Infinite values: 0\n",
      "\n",
      "============================================================\n",
      "SEMANTIC SIMILARITY TEST\n",
      "============================================================\n",
      "\n",
      "Testing semantic similarity between first 3 papers:\n",
      "\n",
      "📄 Paper 0: An Optimal Control View of Adversarial Machine Learning...\n",
      "\n",
      "📄 Paper 1: Machine Learning for Clinical Predictive Analytics...\n",
      "\n",
      "📄 Paper 2: Towards Modular Machine Learning Solution Development: Benef...\n",
      "\n",
      "Similarity matrix:\n",
      "  Paper 0 <-> Paper 0: 1.0000\n",
      "  Paper 0 <-> Paper 1: 0.3258\n",
      "  Paper 0 <-> Paper 2: 0.3027\n",
      "  Paper 1 <-> Paper 0: 0.3258\n",
      "  Paper 1 <-> Paper 1: 1.0000\n",
      "  Paper 1 <-> Paper 2: 0.4468\n",
      "  Paper 2 <-> Paper 0: 0.3027\n",
      "  Paper 2 <-> Paper 1: 0.4468\n",
      "  Paper 2 <-> Paper 2: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================\n",
    "# 4. VALIDATE EMBEDDINGS\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EMBEDDING VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check embedding statistics\n",
    "print(\"\\nEmbedding statistics:\")\n",
    "print(f\"  Mean: {embeddings.mean():.4f}\")\n",
    "print(f\"  Std: {embeddings.std():.4f}\")\n",
    "print(f\"  Min: {embeddings.min():.4f}\")\n",
    "print(f\"  Max: {embeddings.max():.4f}\")\n",
    "\n",
    "# Check for NaN or infinite values\n",
    "print(f\"\\nData quality checks:\")\n",
    "print(f\"  NaN values: {np.isnan(embeddings).sum()}\")\n",
    "print(f\"  Infinite values: {np.isinf(embeddings).sum()}\")\n",
    "\n",
    "# Test semantic similarity\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SEMANTIC SIMILARITY TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Test with first 3 papers\n",
    "test_indices = [0, 1, 2]\n",
    "print(\"\\nTesting semantic similarity between first 3 papers:\")\n",
    "\n",
    "for i in test_indices:\n",
    "    print(f\"\\n📄 Paper {i}: {df.iloc[i]['title'][:60]}...\")\n",
    "\n",
    "print(\"\\nSimilarity matrix:\")\n",
    "for i in test_indices:\n",
    "    for j in test_indices:\n",
    "        sim = cosine_similarity(embeddings[i], embeddings[j])\n",
    "        print(f\"  Paper {i} <-> Paper {j}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e10aba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BUILDING FAISS INDEX\n",
      "============================================================\n",
      "✓ FAISS index created\n",
      "  Index type: IndexFlatIP (Inner Product)\n",
      "  Total vectors: 2130\n",
      "  Dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 5. BUILD FAISS INDEX\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BUILDING FAISS INDEX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "embeddings_normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Create FAISS index (IndexFlatIP for inner product = cosine similarity with normalized vectors)\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "# Add embeddings to index\n",
    "index.add(embeddings_normalized.astype('float32'))\n",
    "\n",
    "print(f\"✓ FAISS index created\")\n",
    "print(f\"  Index type: IndexFlatIP (Inner Product)\")\n",
    "print(f\"  Total vectors: {index.ntotal}\")\n",
    "print(f\"  Dimension: {dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eddabbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING SEMANTIC SEARCH\n",
      "============================================================\n",
      "\n",
      "🔍 Query: 'deep learning for computer vision'\n",
      "------------------------------------------------------------\n",
      "\n",
      "  1. Score: 0.7082\n",
      "     Title: Deep Learning vs. Traditional Computer Vision...\n",
      "     Abstract preview: Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not...\n",
      "\n",
      "  2. Score: 0.6987\n",
      "     Title: Residual Quantity in Percentage of Factory Machines Using Computer Vision and Ma...\n",
      "     Abstract preview: Computer vision has been thriving since AI development was gaining thrust. Using deep learning techniques has been the m...\n",
      "\n",
      "  3. Score: 0.6893\n",
      "     Title: Integration and Performance Analysis of Artificial Intelligence and Computer Vis...\n",
      "     Abstract preview: This paper focuses on the analysis of the application effectiveness of the integration of deep learning and computer vis...\n",
      "\n",
      "🔍 Query: 'natural language processing transformers'\n",
      "------------------------------------------------------------\n",
      "\n",
      "  1. Score: 0.6102\n",
      "     Title: Can Transformers Reason in Fragments of Natural Language?...\n",
      "     Abstract preview: State-of-the-art deep-learning-based approaches to Natural Language Processing (NLP) are credited with various capabilit...\n",
      "\n",
      "  2. Score: 0.5864\n",
      "     Title: Perspectives and Prospects on Transformer Architecture for Cross-Modal Tasks wit...\n",
      "     Abstract preview: Transformer architectures have brought about fundamental changes to computational linguistic field, which had been domin...\n",
      "\n",
      "  3. Score: 0.5842\n",
      "     Title: Natural Language Processing: State of The Art, Current Trends and Challenges...\n",
      "     Abstract preview: Natural language processing (NLP) has recently gained much attention for representing and analysing human language compu...\n",
      "\n",
      "🔍 Query: 'reinforcement learning robotics'\n",
      "------------------------------------------------------------\n",
      "\n",
      "  1. Score: 0.7385\n",
      "     Title: A Concise Introduction to Reinforcement Learning in Robotics...\n",
      "     Abstract preview: One of the biggest hurdles robotics faces is the facet of sophisticated and hard-to-engineer behaviors. Reinforcement le...\n",
      "\n",
      "  2. Score: 0.6663\n",
      "     Title: Boosting Reinforcement Learning and Planning with Demonstrations: A Survey...\n",
      "     Abstract preview: Although reinforcement learning has seen tremendous success recently, this kind of trial-and-error learning can be impra...\n",
      "\n",
      "  3. Score: 0.6596\n",
      "     Title: Backward Learning for Goal-Conditioned Policies...\n",
      "     Abstract preview: Can we learn policies in reinforcement learning without rewards? Can we learn a policy just by trying to reach a goal st...\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 6. TEST SEMANTIC SEARCH\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING SEMANTIC SEARCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def search_papers(query, top_k=5):\n",
    "    \"\"\"Search for papers similar to query\"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    query_normalized = query_embedding / np.linalg.norm(query_embedding)\n",
    "    \n",
    "    # Search\n",
    "    distances, indices = index.search(query_normalized.astype('float32'), top_k)\n",
    "    \n",
    "    return indices[0], distances[0]\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"deep learning for computer vision\",\n",
    "    \"natural language processing transformers\",\n",
    "    \"reinforcement learning robotics\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n🔍 Query: '{query}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    indices, scores = search_papers(query, top_k=3)\n",
    "    \n",
    "    for rank, (idx, score) in enumerate(zip(indices, scores), 1):\n",
    "        paper = df.iloc[idx]\n",
    "        print(f\"\\n  {rank}. Score: {score:.4f}\")\n",
    "        print(f\"     Title: {paper['title'][:80]}...\")\n",
    "        print(f\"     Abstract preview: {paper['abstract_clean'][:120]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5bc8a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING ARTIFACTS\n",
      "============================================================\n",
      "✓ Saved embeddings to ../data/embeddings.npy\n",
      "✓ Saved FAISS index to ../data/faiss_index.bin\n",
      "✓ Saved metadata to ../data/index_metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 7. SAVE ARTIFACTS\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING ARTIFACTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save embeddings\n",
    "np.save('../data/embeddings.npy', embeddings_normalized)\n",
    "print(\"✓ Saved embeddings to ../data/embeddings.npy\")\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index, '../data/faiss_index.bin')\n",
    "print(\"✓ Saved FAISS index to ../data/faiss_index.bin\")\n",
    "\n",
    "# Save metadata for retrieval\n",
    "metadata = {\n",
    "    'model_name': 'all-MiniLM-L6-v2',\n",
    "    'embedding_dim': dimension,\n",
    "    'num_papers': len(df),\n",
    "    'index_type': 'IndexFlatIP',\n",
    "    'creation_date': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('../data/index_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(\"✓ Saved metadata to ../data/index_metadata.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a174d7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SEARCH PERFORMANCE BENCHMARK\n",
      "============================================================\n",
      "\n",
      "Average search latency: 10.52 ms\n",
      "Queries per second: 95.1\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 8. PERFORMANCE BENCHMARKING\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SEARCH PERFORMANCE BENCHMARK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test search latency\n",
    "num_queries = 100\n",
    "query = \"machine learning\"\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(num_queries):\n",
    "    search_papers(query, top_k=10)\n",
    "end = time.time()\n",
    "\n",
    "avg_latency = (end - start) / num_queries\n",
    "\n",
    "print(f\"\\nAverage search latency: {avg_latency*1000:.2f} ms\")\n",
    "print(f\"Queries per second: {1/avg_latency:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bde1fd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EMBEDDING & INDEX CREATION COMPLETE!\n",
      "============================================================\n",
      "Total Papers: 2130\n",
      "Embedding Dimension: 384\n",
      "Model: all-MiniLM-L6-v2\n",
      "Index Type: FAISS IndexFlatIP\n",
      "Avg Search Latency: 10.52 ms\n",
      "Storage Size (embeddings): 3.12 MB\n",
      "\n",
      "✅ Ready for RAG system development in notebook 04\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 9. SUMMARY\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EMBEDDING & INDEX CREATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary = {\n",
    "    'Total Papers': len(df),\n",
    "    'Embedding Dimension': dimension,\n",
    "    'Model': 'all-MiniLM-L6-v2',\n",
    "    'Index Type': 'FAISS IndexFlatIP',\n",
    "    'Avg Search Latency': f\"{avg_latency*1000:.2f} ms\",\n",
    "    'Storage Size (embeddings)': f\"{embeddings_normalized.nbytes / 1024**2:.2f} MB\"\n",
    "}\n",
    "\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\n✅ Ready for RAG system development in notebook 04\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f2b14a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9764d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_en",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
