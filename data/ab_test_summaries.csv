paper_id,title,abstract,summary_a,summary_b,latency_a,latency_b,length_a,length_b
282,A Survey From Distributed Machine Learning to Distributed Deep Learning,"Artificial intelligence has made remarkable progress in handling complex tasks, thanks to advances in hardware acceleration and machine learning algorithms. However, to acquire more accurate outcomes and solve more complex issues, algorithms should be trained with more data. Processing this huge amount of data could be time-consuming and require a great deal of computation. To address these issues, distributed machine learning has been proposed, which involves distributing the data and algorithm across several machines. There has been considerable effort put into developing distributed machine learning algorithms, and different methods have been proposed so far. We divide these algorithms in classification and clustering (traditional machine learning), deep learning and deep reinforcement learning groups. Distributed deep learning has gained more attention in recent years and most of the studies have focused on this approach. Therefore, we mostly concentrate on this category. Based on the investigation of the mentioned algorithms, we highlighted the limitations that should be addressed in future research.","Distributed machine learning involves distributing the data and algorithm across several machines. Distributed deep learning has gained more attention in recent years. We divide these algorithms in classification and clustering, deep learning and deep reinforcement learning groups."," Distributed machine learning involves distributing the data and algorithm across several machines . Distributed deep learning has gained more attention in recent years . We divide these algorithms in classification and clustering (traditional machine learning), deep learning and deep reinforcement learning groups .",2.1619834899902344,1.282689094543457,37,43
2007,Towards Optimal Head-to-head Autonomous Racing with Curriculum Reinforcement Learning,"Head-to-head autonomous racing is a challenging problem, as the vehicle needs to operate at the friction or handling limits in order to achieve minimum lap times while also actively looking for strategies to overtake/stay ahead of the opponent. In this work we propose a head-to-head racing environment for reinforcement learning which accurately models vehicle dynamics. Some previous works have tried learning a policy directly in the complex vehicle dynamics environment but have failed to learn an optimal policy. In this work, we propose a curriculum learning-based framework by transitioning from a simpler vehicle model to a more complex real environment to teach the reinforcement learning agent a policy closer to the optimal policy. We also propose a control barrier function-based safe reinforcement learning algorithm to enforce the safety of the agent in a more effective way while not compromising on optimality.",We propose a head-to-head racing environment for reinforcement learning which accurately models vehicle dynamics. We also propose a control barrier function-based safe reinforcement learning algorithm., In this work we propose a head-to-head racing environment for reinforcement learning which accurately models vehicle dynamics . We also propose a control barrier function-based safe reinforcement learning algorithm .,1.8711872100830078,1.1448860168457031,25,30
1712,PoBRL: Optimizing Multi-Document Summarization by Blending Reinforcement Learning Policies,"We propose a novel reinforcement learning based framework PoBRL for solving multi-document summarization. PoBRL jointly optimizes over the following three objectives necessary for a high-quality summary: importance, relevance, and length. Our strategy decouples this multi-objective optimization into different subproblems that can be solved individually by reinforcement learning. Utilizing PoBRL, we then blend each learned policies together to produce a summary that is a concise and complete representation of the original input. Our empirical analysis shows state-of-the-art performance on several multi-document datasets. Human evaluation also shows that our method produces high-quality output.","PoBRL jointly optimizes over the following three objectives necessary for a high-quality summary: importance, relevance, and length. Our strategy decouples this multi-objective optimization into different subproblems that can be solved individually by reinforcement learning."," PoBRL jointly optimizes over the three objectives necessary for a high-quality summary: importance, relevance, and length . Our strategy decouples this multi-objective optimization into different subproblems that can be solved individually by reinforcement learning .",2.465726613998413,1.6536688804626465,34,35
1665,A Novel Update Mechanism for Q-Networks Based On Extreme Learning Machines,"Reinforcement learning is a popular machine learning paradigm which can find near optimal solutions to complex problems. Most often, these procedures involve function approximation using neural networks with gradient based updates to optimise weights for the problem being considered. While this common approach generally works well, there are other update mechanisms which are largely unexplored in reinforcement learning. One such mechanism is Extreme Learning Machines. These were initially proposed to drastically improve the training speed of neural networks and have since seen many applications. Here we attempt to apply extreme learning machines to a reinforcement learning problem in the same manner as gradient based updates. This new algorithm is called Extreme Q-Learning Machine (EQLM). We compare its performance to a typical Q-Network on the cart-pole task - a benchmark reinforcement learning problem - and show EQLM has similar long-term learning performance to a Q-Network.",Reinforcement learning is a popular machine learning paradigm which can find near optimal solutions to complex problems. We attempt to apply extreme learning machines to a reinforcement learning problem in the same manner as gradient based updates. This new algorithm is called Extreme Q-Learning Machine (EQLM) We compare its performance to a typical Q-Network on the cart-pole task., Reinforcement learning is a popular machine learning paradigm which can find near optimal solutions to complex problems . Here we attempt to apply extreme learning machines to a reinforcement learning problem in the same manner as gradient based updates . We compare its performance to a typical Q-Network on the cart-pole task .,2.2529118061065674,1.6471481323242188,58,53
2026,L2Explorer: A Lifelong Reinforcement Learning Assessment Environment,"Despite groundbreaking progress in reinforcement learning for robotics, gameplay, and other complex domains, major challenges remain in applying reinforcement learning to the evolving, open-world problems often found in critical application spaces. Reinforcement learning solutions tend to generalize poorly when exposed to new tasks outside of the data distribution they are trained on, prompting an interest in continual learning algorithms. In tandem with research on continual learning algorithms, there is a need for challenge environments, carefully designed experiments, and metrics to assess research progress. We address the latter need by introducing a framework for continual reinforcement-learning development and assessment using Lifelong Learning Explorer (L2Explorer), a new, Unity-based, first-person 3D exploration environment that can be continuously reconfigured to generate a range of tasks and task variants structured into complex and evolving evaluation curricula. In contrast to procedurally generated worlds with randomized components, we have developed a systematic approach to defining curricula in response to controlled changes with accompanying metrics to assess transfer, performance recovery, and data efficiency. Taken together, the L2Explorer environment and evaluation approach provides a framework for developing future evaluation methodologies in open-world settings and rigorously evaluating approaches to lifelong learning.","Reinforcement learning solutions tend to generalize poorly when exposed to new tasks outside of the data distribution they are trained on. There is a need for challenge environments, carefully designed experiments, and metrics to assess research progress. We introduce a framework for continual reinforcement-learning development and assessment using Lifelong Learning Explorer."," Despite groundbreaking progress in reinforcement learning, major challenges remain in applying reinforcement learning to the evolving, open-world problems often found in critical application spaces . Reinforcement learning solutions tend to generalize poorly when exposed to new tasks outside of the data distribution they are trained on . In tandem with research on continual learning algorithms, there is a need for challenge environments, carefully designed experiments, and metrics to assess research progress .",1.8762774467468262,2.0590457916259766,51,72
297,Kernel Machines With Missing Responses,"Missing responses is a missing data format in which outcomes are not always observed. In this work we develop kernel machines that can handle missing responses. First, we propose a kernel machine family that uses mainly the complete cases. For the quadratic loss, we then propose a family of doubly-robust kernel machines. The proposed kernel-machine estimators can be applied to both regression and classification problems. We prove oracle inequalities for the finite-sample differences between the kernel machine risk and Bayes risk. We use these oracle inequalities to prove consistency and to calculate convergence rates. We demonstrate the performance of the two proposed kernel machine families using both a simulation study and a real-world data analysis.",Missing responses is a missing data format in which outcomes are not always observed. In this work we develop kernel machines that can handle missing responses. The proposed kernel-machine estimators can be applied to both regression and classification problems., Missing responses is a missing data format in which outcomes are not always observed . In this work we develop kernel machines that can handle missing responses . The proposed kernel-machine estimators can be applied to both regression and classification problems .,2.155827283859253,1.559680461883545,39,42
1744,Generalization in Transfer Learning,"Agents trained with deep reinforcement learning algorithms are capable of performing highly complex tasks including locomotion in continuous environments. We investigate transferring the learning acquired in one task to a set of previously unseen tasks. Generalization and overfitting in deep reinforcement learning are not commonly addressed in current transfer learning research. Conducting a comparative analysis without an intermediate regularization step results in underperforming benchmarks and inaccurate algorithm comparisons due to rudimentary assessments. In this study, we propose regularization techniques in deep reinforcement learning for continuous control through the application of sample elimination, early stopping and maximum entropy regularized adversarial learning. First, the importance of the inclusion of training iteration number to the hyperparameters in deep transfer reinforcement learning will be discussed. Because source task performance is not indicative of the generalization capacity of the algorithm, we start by acknowledging the training iteration number as a hyperparameter. In line with this, we introduce an additional step of resorting to earlier snapshots of policy parameters to prevent overfitting to the source task. Then, to generate robust policies, we discard the samples that lead to overfitting via a method we call strict clipping. Furthermore, we increase the generalization capacity in widely used transfer learning benchmarks by using maximum entropy regularization, different critic methods, and curriculum learning in an adversarial setup. Subsequently, we propose maximum entropy adversarial reinforcement learning to increase the domain randomization. Finally, we evaluate the robustness of these methods on simulated robots in target environments where the morphology of the robot, gravity, and tangential friction coefficient of the environment are altered.","Generalization and overfitting in deep reinforcement learning are not commonly addressed in current transfer learning research. Conducting a comparative analysis without an intermediate regularization step results in underperforming benchmarks and inaccurate algorithm comparisons. We propose regularization techniques for continuous control through the application of sample elimination, early stopping and maximum entropy regularized adversarial learning.", Agents trained with deep reinforcement learning algorithms are capable of performing highly complex tasks including locomotion in continuous environments . We investigate transferring the learning acquired in one task to a set of previously unseen tasks . We propose regularization techniques in deep . reinforcement learning for continuous control through the application of sample elimination and early stopping .,3.0877485275268555,1.6894559860229492,54,59
1697,A Reinforcement Learning Perspective on the Optimal Control of Mutation Probabilities for the (1+1) Evolutionary Algorithm: First Results on the OneMax Problem,"We study how Reinforcement Learning can be employed to optimally control parameters in evolutionary algorithms. We control the mutation probability of a (1+1) evolutionary algorithm on the OneMax function. This problem is modeled as a Markov Decision Process and solved with Value Iteration via the known transition probabilities. It is then solved via Q-Learning, a Reinforcement Learning algorithm, where the exact transition probabilities are not needed. This approach also allows previous expert or empirical knowledge to be included into learning. It opens new perspectives, both formally and computationally, for the problem of parameter control in optimization.","We study how Reinforcement Learning can be employed to optimally control parameters in evolutionary algorithms. This approach also allows previous expert or empirical knowledge to be included into learning. It opens new perspectives, both formally and computationally, for the problem of parameter control in optimization.", We study how Reinforcement Learning can be employed to optimally control parameters in evolutionary algorithms . We control the mutation probability of a (1+1) evolutionary algorithm on the OneMax function . This approach also allows previous expert knowledge to be included into learning .,2.3516597747802734,1.5548837184906006,45,44
70,Challenges and Opportunities in Quantum Machine Learning,"At the intersection of machine learning and quantum computing, Quantum Machine Learning (QML) has the potential of accelerating data analysis, especially for quantum data, with applications for quantum materials, biochemistry, and high-energy physics. Nevertheless, challenges remain regarding the trainability of QML models. Here we review current methods and applications for QML. We highlight differences between quantum and classical machine learning, with a focus on quantum neural networks and quantum deep learning. Finally, we discuss opportunities for quantum advantage with QML.","Quantum Machine Learning (QML) has the potential of accelerating data analysis, especially for quantum data. It has applications for quantum materials, biochemistry, and high-energy physics. Here we review current methods and applications for QML."," Quantum Machine Learning (QML) has the potential of accelerating data analysis, especially for quantum data . We highlight differences between quantum and classical machine learning, with a focus on quantum neural networks and quantum deep learning . We discuss opportunities for quantum advantage with QML .",1.8594989776611328,1.4508891105651855,34,46
290,Partial differential equation regularization for supervised machine learning,"This article is an overview of supervised machine learning problems for regression and classification. Topics include: kernel methods, training by stochastic gradient descent, deep learning architecture, losses for classification, statistical learning theory, and dimension independent generalization bounds. Implicit regularization in deep learning examples are presented, including data augmentation, adversarial training, and additive noise. These methods are reframed as explicit gradient regularization.","This article is an overview of supervised machine learning problems for regression and classification. Topics include: kernel methods, training by stochastic gradient descent, deep learning architecture, losses for classification, statistical learning theory, and dimension independent generalization bounds. Implicit regularization in deep learning examples are presented, including data augmentation, adversarial training, and additive noise."," This article is an overview of supervised machine learning problems for regression and classification . Topics include: kernel methods, training by stochastic gradient descent, deep learning architecture, losses for classification, statistical learning theory, and dimension independent generalization bounds . Implicit regularization in deep learning examples are presented .",2.673691511154175,2.264564275741577,53,48
1720,Vanishing Bias Heuristic-guided Reinforcement Learning Algorithm,"Reinforcement Learning has achieved tremendous success in the many Atari games. In this paper we explored with the lunar lander environment and implemented classical methods including Q-Learning, SARSA, MC as well as tiling coding. We also implemented Neural Network based methods including DQN, Double DQN, Clipped DQN. On top of these, we proposed a new algorithm called Heuristic RL which utilizes heuristic to guide the early stage training while alleviating the introduced human bias. Our experiments showed promising results for our proposed methods in the lunar lander environment.","Reinforcement Learning has achieved tremendous success in the many Atari games. In this paper we explored with the lunar lander environment. We implemented classical methods including Q-Learning, SARSA, MC as well as tiling coding. We also implemented Neural Network based methods including DQN. On top of these, we proposed a new algorithm called Heuristic RL."," Reinforcement Learning has achieved tremendous success in the many Atari games . In this paper we explored with the lunar lander environment and implemented classical methods including Q-Learning, SARSA, MC as well as tiling coding . We also implemented Neural Network based methods .",2.0819592475891113,1.8337528705596924,55,44
1916,Learning through Probing: a decentralized reinforcement learning architecture for social dilemmas,"Multi-agent reinforcement learning has received significant interest in recent years notably due to the advancements made in deep reinforcement learning which have allowed for the developments of new architectures and learning algorithms. Using social dilemmas as the training ground, we present a novel learning architecture, Learning through Probing (LTP), where agents utilize a probing mechanism to incorporate how their opponent's behavior changes when an agent takes an action. We use distinct training phases and adjust rewards according to the overall outcome of the experiences accounting for changes to the opponents behavior. We introduce a parameter eta to determine the significance of these future changes to opponent behavior. When applied to the Iterated Prisoner's Dilemma (IPD), LTP agents demonstrate that they can learn to cooperate with each other, achieving higher average cumulative rewards than other reinforcement learning methods while also maintaining good performance in playing against static agents that are present in Axelrod tournaments. We compare this method with traditional reinforcement learning algorithms and agent-tracking techniques to highlight key differences and potential applications. We also draw attention to the differences between solving games and societal-like interactions and analyze the training of Q-learning agents in makeshift societies. This is to emphasize how cooperation may emerge in societies and demonstrate this using environments where interactions with opponents are determined through a random encounter format of the IPD.","Multi-agent reinforcement learning has received significant interest in recent years. Using social dilemmas as the training ground, we present a novel learning architecture, Learning through Probing (LTP) LTP agents demonstrate that they can learn to cooperate with each other, achieving higher average cumulative rewards."," Multi-agent reinforcement learning has received significant interest in recent years notably due to the advancements made in deep reinforcement learning . We present a novel learning architecture, Learning through Probing (LTP) where agents utilize a probing mechanism to incorporate how their opponent's behavior changes when an agent takes an action . We use distinct training phases and adjust rewards according to the overall outcome of the experiences accounting for changes to the opponents behavior . LTP agents demonstrate they can learn to cooperate with each other, achieving higher average cumulative rewards than other reinforcement learning methods .",2.867601156234741,2.473233461380005,44,97
507,Deep Active Learning by Leveraging Training Dynamics,"Active learning theories and methods have been extensively studied in classical statistical learning settings. However, deep active learning, i.e., active learning with deep learning models, is usually based on empirical criteria without solid theoretical justification, thus suffering from heavy doubts when some of those fail to provide benefits in real applications. In this paper, by exploring the connection between the generalization performance and the training dynamics, we propose a theory-driven deep active learning method (dynamicAL) which selects samples to maximize training dynamics. In particular, we prove that the convergence speed of training and the generalization performance are positively correlated under the ultra-wide condition and show that maximizing the training dynamics leads to better generalization performance. Furthermore, to scale up to large deep neural networks and data sets, we introduce two relaxations for the subset selection problem and reduce the time complexity from polynomial to constant. Empirical results show that dynamicAL not only outperforms the other baselines consistently but also scales well on large deep learning models. We hope our work would inspire more attempts on bridging the theoretical findings of deep networks and practical impacts of deep active learning in real applications.","Active learning theories and methods have been extensively studied in classical statistical learning settings. However, deep active learning, i.e., active learning with deep learning models, is usually based on empirical criteria without solid theoretical justification."," Active learning theories and methods have been extensively studied in classical statistical learning settings . However, active learning with deep learning models is usually based on empirical criteria without solid theoretical justification .",1.861201286315918,1.4526002407073975,35,33
650,A Survey of Forex and Stock Price Prediction Using Deep Learning,"The prediction of stock and foreign exchange (Forex) had always been a hot and profitable area of study. Deep learning application had proven to yields better accuracy and return in the field of financial prediction and forecasting. In this survey we selected papers from the DBLP database for comparison and analysis. We classified papers according to different deep learning methods, which included: Convolutional neural network (CNN), Long Short-Term Memory (LSTM), Deep neural network (DNN), Recurrent Neural Network (RNN), Reinforcement Learning, and other deep learning methods such as HAN, NLP, and Wavenet. Furthermore, this paper reviewed the dataset, variable, model, and results of each article. The survey presented the results through the most used performance metrics: RMSE, MAPE, MAE, MSE, accuracy, Sharpe ratio, and return rate. We identified that recent models that combined LSTM with other methods, for example, DNN, are widely researched. Reinforcement learning and other deep learning method yielded great returns and performances. We conclude that in recent years the trend of using deep-learning based method for financial modeling is exponentially rising.",Deep learning application had proven to yields better accuracy and return in the field of financial prediction and forecasting. In recent years the trend of using deep-learning based method for financial modeling is exponentially rising., The prediction of stock and foreign exchange (Forex) had always been a hot and profitable area of study . Deep learning application had proven to yields better accuracy and return in the field of financial prediction and forecasting . The trend of using deep-learning based method for financial modeling is exponentially rising .,2.4761457443237305,2.6814539432525635,35,53
916,Supporting Undotted Arabic with Pre-trained Language Models,"We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithms. Content classification is typically done by fine-tuning pre-trained language models, which have been recently employed by many natural-language-processing applications. In this work we study the effect of applying pre-trained Arabic language models on ""undotted"" Arabic texts. We suggest several ways of supporting undotted texts with pre-trained models, without additional training, and measure their performance on two Arabic natural-language-processing downstream tasks. The results are encouraging; in one of the tasks our method shows nearly perfect performance.","Social media users intentionally remove consonantal dots from Arabic letters to bypass content-classification algorithms. Content classification is typically done by fine-tuning pre-trained language models, which have been recently employed by many natural-language-processing applications. We suggest several ways of supporting undotted texts with pre- trained models, without additional training.", Social media users intentionally remove consonantal dots from Arabic letters in order to bypass content-classification algorithms . Pre-trained Arabic language models have been recently employed by many natural-language-processing applications . We suggest several ways of supporting undotted texts with pre-trained models .,2.680380344390869,1.8608756065368652,48,42
450,Learning Task-aware Robust Deep Learning Systems,"Many works demonstrate that deep learning system is vulnerable to adversarial attack. A deep learning system consists of two parts: the deep learning task and the deep model. Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning task. In this paper, we adopt the binary and interval label encoding strategy to redefine the classification task and design corresponding loss to improve robustness of the deep learning system. Our method can be viewed as improving the robustness of deep learning systems from both the learning task and deep model. Experimental results demonstrate that our learning task-aware method is much more robust than traditional classification while retaining the accuracy.","Deep learning system consists of two parts: the deep learning task and the deep model. Many works demonstrate that deep learning system is vulnerable to adversarial attack. In this paper, we adopt the binary and interval label encoding strategy to redefine the classification task.", Many works demonstrate that deep learning system is vulnerable to adversarial attack . Our method can be viewed as improving the robustness of deep learning systems from both the learning task and deep model . Experimental results demonstrate that our learning task-aware method is much more robust .,2.168161630630493,1.6460697650909424,44,48
1103,Challenges and Opportunities for Computer Vision in Real-life Soccer Analytics,"In this paper, we explore some of the applications of computer vision to sports analytics. Sport analytics deals with understanding and discovering patterns from a corpus of sports data. Analysing such data provides important performance metrics for the players, for instance in soccer matches, that could be useful for estimating their fitness and strengths. Team level statistics can also be estimated from such analysis. This paper mainly focuses on some the challenges and opportunities presented by sport video analysis in computer vision. Specifically, we use our multi-camera setup as a framework to discuss some of the real-life challenges for machine learning algorithms.",Sport analytics deals with understanding and discovering patterns from a corpus of sports data. Analysing such data provides important performance metrics for the players. This paper mainly focuses on some the challenges and opportunities presented by sport video analysis in computer vision., Sport analytics deals with understanding and discovering patterns from a corpus of sports data . Analysing such data provides important performance metrics for the players . This paper mainly focuses on some the challenges and opportunities presented by computer vision in computer vision .,2.1524479389190674,1.7490239143371582,42,44
1788,A Distributional Perspective on Reinforcement Learning,"In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.","This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour."," In this paper we argue for the fundamental importance of the value distribution . This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value . We use the distributional perspective to design a new algorithm which applies Bellman's equation to learning of approximate value distributions .",2.1662943363189697,1.6538150310516357,48,56
196,MetaPhysiCa: OOD Robustness in Physics-informed Machine Learning,"A fundamental challenge in physics-informed machine learning (PIML) is the design of robust PIML methods for out-of-distribution (OOD) forecasting tasks. These OOD tasks require learning-to-learn from observations of the same (ODE) dynamical system with different unknown ODE parameters, and demand accurate forecasts even under out-of-support initial conditions and out-of-support ODE parameters. In this work we propose a solution for such tasks, which we define as a meta-learning procedure for causal structure discovery (including invariant risk minimization). Using three different OOD tasks, we empirically observe that the proposed approach significantly outperforms existing state-of-the-art PIML and deep learning methods.","A fundamental challenge in physics-informed machine learning (PIML) is the design of robust PIML methods for out-of-distribution (OOD) forecasting tasks. These OOD tasks require learning-to-learn from observations of the same (ODE) dynamical system with different unknown ODE parameters. We propose a solution for such tasks, which we define as a meta-learning procedure for causal structure discovery (including invariant risk minimization)", A fundamental challenge in physics-informed machine learning (PIML) is the design of robust PIML methods for out-of-distribution (OOD) forecasting tasks . These OOD tasks require learning-to-learn from observations of the same (ODE) dynamical system with different unknown ODE parameters .,2.4775960445404053,1.75677490234375,60,40
109,Learnable: Theory vs Applications,"Two different views on machine learning problem: Applied learning (machine learning with business applications) and Agnostic PAC learning are formalized and compared here. I show that, under some conditions, the theory of PAC Learnable provides a way to solve the Applied learning problem. However, the theory requires to have the training sets so large, that it would make the learning practically useless. I suggest shedding some theoretical misconceptions about learning to make the theory more aligned with the needs and experience of practitioners.","Theory of PAC Learnable provides a way to solve the Applied learning problem. However, the theory requires to have the training sets so large, that it would make the learning practically useless."," The theory of PAC Learnable provides a way to solve the Applied learning problem . However, the theory requires to have the training sets so large, that it would make the learning practically useless .",1.3457753658294678,1.2421166896820068,32,35
1731,Reinforcement learning,"Observing celestial objects and advancing our scientific knowledge about them involves tedious planning, scheduling, data collection and data post-processing. Many of these operational aspects of astronomy are guided and executed by expert astronomers. Reinforcement learning is a mechanism where we (as humans and astronomers) can teach agents of artificial intelligence to perform some of these tedious tasks. In this paper, we will present a state of the art overview of reinforcement learning and how it can benefit astronomy.","Reinforcement learning is a mechanism where we (as humans and astronomers) can teach agents of artificial intelligence to perform some of these tedious tasks. In this paper, we will present a state of the art overview of reinforcement learning and how it can benefit astronomy."," Observing celestial objects involves tedious planning, scheduling, data collection and data post-processing . Reinforcement learning is a mechanism where we (as humans and astronomers) can teach agents of artificial intelligence to perform these tasks .",2.3629581928253174,1.174485206604004,45,35
647,The Unreasonable Effectiveness of Deep Learning in Artificial Intelligence,"Deep learning networks have been trained to recognize speech, caption photographs and translate text between languages at high levels of performance. Although applications of deep learning networks to real world problems have become ubiquitous, our understanding of why they are so effective is lacking. These empirical results should not be possible according to sample complexity in statistics and non-convex optimization theory. However, paradoxes in the training and effectiveness of deep learning networks are being investigated and insights are being found in the geometry of high-dimensional spaces. A mathematical theory of deep learning would illuminate how they function, allow us to assess the strengths and weaknesses of different network architectures and lead to major improvements. Deep learning has provided natural ways for humans to communicate with digital devices and is foundational for building artificial general intelligence. Deep learning was inspired by the architecture of the cerebral cortex and insights into autonomy and general intelligence may be found in other brain regions that are essential for planning and survival, but major breakthroughs will be needed to achieve these goals.","Deep learning networks have been trained to recognize speech, caption photographs and translate text between languages at high levels of performance. Although applications of deep learning networks to real world problems have become ubiquitous, our understanding of why they are so effective is lacking."," Deep learning networks have been trained to recognize speech, caption photographs and translate text between languages at high levels of performance . A mathematical theory of deep learning would illuminate how they function and lead to major improvements .",1.8246374130249023,1.237239122390747,44,39
298,Transformative Machine Learning,"The key to success in machine learning (ML) is the use of effective data representations. Traditionally, data representations were hand-crafted. Recently it has been demonstrated that, given sufficient data, deep neural networks can learn effective implicit representations from simple input representations. However, for most scientific problems, the use of deep learning is not appropriate as the amount of available data is limited, and/or the output models must be explainable. Nevertheless, many scientific problems do have significant amounts of data available on related tasks, which makes them amenable to multi-task learning, i.e. learning many related problems simultaneously. Here we propose a novel and general representation learning approach for multi-task learning that works successfully with small amounts of data. The fundamental new idea is to transform an input intrinsic data representation (i.e., handcrafted features), to an extrinsic representation based on what a pre-trained set of models predict about the examples. This transformation has the dual advantages of producing significantly more accurate predictions, and providing explainable models. To demonstrate the utility of this transformative learning approach, we have applied it to three real-world scientific problems: drug-design (quantitative structure activity relationship learning), predicting human gene expression (across different tissue types and drug treatments), and meta-learning for machine learning (predicting which machine learning methods work best for a given problem). In all three problems, transformative machine learning significantly outperforms the best intrinsic representation.","The key to success in machine learning is the use of effective data representations. Traditionally, data representations were hand-crafted. We propose a novel and general representation learning approach for multi-task learning that works successfully with small amounts of data."," The key to success in machine learning is the use of effective data representations . Traditionally, data representations were hand-crafted . We propose a novel and general representation learning approach for multi-task learning .",2.471099615097046,2.0643041133880615,39,34
1954,Information Maximizing Exploration with a Latent Dynamics Model,"All reinforcement learning algorithms must handle the trade-off between exploration and exploitation. Many state-of-the-art deep reinforcement learning methods use noise in the action selection, such as Gaussian noise in policy gradient methods or $\epsilon$-greedy in Q-learning. While these methods are appealing due to their simplicity, they do not explore the state space in a methodical manner. We present an approach that uses a model to derive reward bonuses as a means of intrinsic motivation to improve model-free reinforcement learning. A key insight of our approach is that this dynamics model can be learned in the latent feature space of a value function, representing the dynamics of the agent and the environment. This method is both theoretically grounded and computationally advantageous, permitting the efficient use of Bayesian information-theoretic methods in high-dimensional state spaces. We evaluate our method on several continuous control tasks, focusing on improving exploration.",Many state-of-the-art deep reinforcement learning methods use noise in the action selection. We present an approach that uses a model to derive reward bonuses as a means of intrinsic motivation to improve model-free reinforcement learning. This method is both theoretically grounded and computationally advantageous.," All reinforcement learning algorithms must handle the trade-off between exploration and exploitation . We present an approach that uses a model to derive reward bonuses as a means of intrinsic motivation to improve model-free reinforcement learning . We evaluate our method on several continuous control tasks, focusing on improving exploration .",2.273388147354126,1.3194308280944824,44,51
1572,KF-LAX: Kronecker-factored curvature estimation for control variate optimization in reinforcement learning,"A key challenge for gradient based optimization methods in model-free reinforcement learning is to develop an approach that is sample efficient and has low variance. In this work, we apply Kronecker-factored curvature estimation technique (KFAC) to a recently proposed gradient estimator for control variate optimization, RELAX, to increase the sample efficiency of using this gradient estimation method in reinforcement learning. The performance of the proposed method is demonstrated on a synthetic problem and a set of three discrete control task Atari games.","A key challenge for gradient based optimization methods in model-free reinforcement learning is to develop an approach that is sample efficient and has low variance. In this work, we apply Kronecker-factored curvature estimation technique (KFAC) to a recently proposed gradient estimator. The performance of the proposed method is demonstrated on a synthetic problem and a set of three discrete control task Atari games."," A key challenge for gradient based optimization methods in model-free reinforcement learning is to develop an approach that is sample efficient and has low variance . We apply Kronecker-factored curvature estimation technique (KFAC) to a recently proposed gradient estimator for control variate optimization, RELAX .",2.9993479251861572,1.761131763458252,63,45
